{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p>"},{"location":"#demo","title":"Demo","text":"<p>Try <code>any-llm</code> in action with our interactive chat demo that showcases streaming completions and provider switching:</p> <p>\ud83d\udcc2 Run the Demo</p> <p>The demo features real-time streaming responses, multiple provider support, and collapsible \"thinking\" content display.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#parameters","title":"Parameters","text":"<p>For a complete list of available functions and their parameters, see the completion, embedding, and responses API documentation.</p>"},{"location":"#error-handling","title":"Error Handling","text":"<p><code>any-llm</code> provides custom exceptions to indicate common errors like missing API keys and parameters that are unsupported by a specific provider.</p> <p>For more details on exceptions, see the exceptions API documentation.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the below providers. In order to discover information about what models are supported by a provider as well as what features the provider supports for each model, refer to the provider documentation.</p> <p>Legend</p> <ul> <li>Reasoning (Completions): Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate \"reasoning models\".See this</li> <li>Streaming (Completions): Provider can stream completion results back as an iterator. discussion for more information.</li> <li>Responses API: Provider supports the Responses API variant for text generation.  See this to follow along with our implementation effort.</li> <li>List Models API: Provider supports listing available models programmatically via the <code>list_models()</code> function. This allows you to discover what models are available from the provider at runtime, which can be useful for dynamic model selection or validation.</li> </ul> ID Env Var Source Code Responses Completion Streaming(Completions) Reasoning(Completions) Embedding List Models <code>anthropic</code> ANTHROPIC_API_KEY Source \u274c \u2705 \u2705 \u2705 \u274c \u2705 <code>aws</code> AWS_BEARER_TOKEN_BEDROCK Source \u274c \u2705 \u2705 \u274c \u2705 \u2705 <code>azure</code> AZURE_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 \u274c <code>azureopenai</code> AZURE_OPENAI_API_KEY Source \u2705 \u2705 \u2705 \u274c \u2705 \u2705 <code>cerebras</code> CEREBRAS_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>cohere</code> CO_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>databricks</code> DATABRICKS_TOKEN Source \u274c \u2705 \u2705 \u274c \u2705 \u2705 <code>deepseek</code> DEEPSEEK_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>fireworks</code> FIREWORKS_API_KEY Source \u2705 \u2705 \u2705 \u274c \u274c \u2705 <code>google</code> GOOGLE_API_KEY/GEMINI_API_KEY Source \u274c \u2705 \u2705 \u2705 \u2705 \u2705 <code>groq</code> GROQ_API_KEY Source \u2705 \u2705 \u2705 \u2705 \u274c \u2705 <code>huggingface</code> HF_TOKEN Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>inception</code> INCEPTION_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>llama</code> LLAMA_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>llamacpp</code> LLAMA_API_KEY Source \u274c \u2705 \u274c \u274c \u2705 \u2705 <code>llamafile</code> None Source \u274c \u2705 \u274c \u274c \u274c \u2705 <code>lmstudio</code> LM_STUDIO_API_KEY Source \u274c \u2705 \u2705 \u2705 \u2705 \u2705 <code>mistral</code> MISTRAL_API_KEY Source \u274c \u2705 \u2705 \u2705 \u2705 \u2705 <code>moonshot</code> MOONSHOT_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>nebius</code> NEBIUS_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 \u2705 <code>ollama</code> None Source \u274c \u2705 \u2705 \u2705 \u2705 \u2705 <code>openai</code> OPENAI_API_KEY Source \u2705 \u2705 \u2705 \u274c \u2705 \u2705 <code>openrouter</code> OPENROUTER_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>perplexity</code> PERPLEXITY_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>portkey</code> PORTKEY_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>sambanova</code> SAMBANOVA_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 \u2705 <code>together</code> TOGETHER_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>voyage</code> VOYAGE_API_KEY Source \u274c \u274c \u274c \u274c \u2705 \u274c <code>watsonx</code> WATSONX_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c \u2705 <code>xai</code> XAI_API_KEY Source \u274c \u2705 \u2705 \u2705 \u274c \u2705"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>API_KEYS to access to whichever LLM you choose to use.</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#direct-usage","title":"Direct Usage","text":"<p>In your pip install, include the supported providers that you plan on using, or use the <code>all</code> option if you want to install support for all <code>any-llm</code> supported providers.</p> <pre><code>pip install any-llm-sdk[mistral]  # For Mistral provider\npip install any-llm-sdk[ollama]   # For Ollama provider\n# install multiple providers\npip install any-llm-sdk[mistral,ollama]\n# or install support for all providers\npip install any-llm-sdk[all]\n</code></pre>"},{"location":"quickstart/#library-integration","title":"Library Integration","text":"<p>If you're integrating <code>any-llm</code> into your own library that others will use, you only need to install the base package:</p> <pre><code>pip install any-llm-sdk\n</code></pre> <p>In this scenario, the end users of your library will be responsible for installing the appropriate provider dependencies when they want to use specific providers. <code>any-llm</code> is designed so that you'll only encounter exceptions at runtime if you try to use a provider without having the required dependencies installed.</p> <p>Those exceptions will clearly describe what needs to be installed to resolve the issue.</p> <p>Make sure you have the appropriate API key environment variable set for your provider. Alternatively, you could use the <code>api_key</code> parameter when making a completion call instead of setting an environment variable.</p> <pre><code>export MISTRAL_API_KEY=\"YOUR_KEY_HERE\"  # or OPENAI_API_KEY, etc\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>completion</code> and <code>acompletion</code> use a unified interface across all providers.</p> <p>Recommended approach: Use separate <code>provider</code> and <code>model</code> parameters:</p> <pre><code>import os\n\nfrom any_llm import completion, ProviderName\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\n# Recommended: separate provider and model parameters\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\", # or ProviderName.MISTRAL\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre> <p>Alternative syntax: You can also use the combined <code>provider:model</code> format:</p> <pre><code>response = completion(\n    model=\"mistral:mistral-small-latest\",  # &lt;provider_id&gt;:&lt;model_id&gt;\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <p>The provider_id should be specified according to the provider ids supported by any-llm. The <code>model_id</code> portion is passed directly to the provider internals: to understand what model ids are available for a provider, you will need to refer to the provider documentation or use our <code>list_models</code>  API if the provider supports that API.</p>"},{"location":"quickstart/#streaming","title":"Streaming","text":"<p>For the providers that support streaming, you can enable it by passing <code>stream=True</code>:</p> <pre><code>output = \"\"\nfor chunk in completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n):\n    chunk_content = chunk.choices[0].delta.content or \"\"\n    print(chunk_content)\n    output += chunk_content\n</code></pre>"},{"location":"quickstart/#embeddings","title":"Embeddings","text":"<p><code>embedding</code> and <code>aembedding</code> allow you to create vector embeddings from text using the same unified interface across providers.</p> <p>Not all providers support embeddings - check the providers documentation to see which ones do.</p> <pre><code>from any_llm import embedding\n\nresult = embedding(\n    model=\"text-embedding-3-small\",\n    provider=\"openai\",\n    inputs=\"Hello, world!\" # can be either string or list of strings\n)\n\n# Access the embedding vector\nembedding_vector = result.data[0].embedding\nprint(f\"Embedding vector length: {len(embedding_vector)}\")\nprint(f\"Tokens used: {result.usage.total_tokens}\")\n</code></pre>"},{"location":"quickstart/#tools","title":"Tools","text":"<p><code>any-llm</code> supports tool calling for providers that support it. You can pass a list of tools where each tool is either:</p> <ol> <li>Python callable - Functions with proper docstrings and type annotations</li> <li>OpenAI Format tool dict - Already in OpenAI tool format</li> </ol> <pre><code>from any_llm import completion\n\ndef get_weather(location: str, unit: str = \"F\") -&gt; str:\n    \"\"\"Get weather information for a location.\n\n    Args:\n        location: The city or location to get weather for\n        unit: Temperature unit, either 'C' or 'F'\n    \"\"\"\n    return f\"Weather in {location} is sunny and 75{unit}!\"\n\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Pittsburgh PA?\"}],\n    tools=[get_weather]\n)\n</code></pre> <p>any-llm automatically converts your Python functions to OpenAI tools format. Functions must have: - A docstring describing what the function does - Type annotations for all parameters - A return type annotation</p>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.completion","title":"<code>any_llm.completion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, api_timeout=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | ProviderName | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    provider_instance, completion_params = _process_completion_params(\n        model=model,\n        provider=provider,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        api_key=api_key,\n        api_base=api_base,\n        api_timeout=api_timeout,\n        user=user,\n        parallel_tool_calls=parallel_tool_calls,\n        logprobs=logprobs,\n        top_logprobs=top_logprobs,\n        logit_bias=logit_bias,\n        stream_options=stream_options,\n        max_completion_tokens=max_completion_tokens,\n        reasoning_effort=reasoning_effort,\n        client_args=client_args,\n        **kwargs,\n    )\n\n    return provider_instance.completion(completion_params, **kwargs)\n</code></pre>"},{"location":"api/completion/#any_llm.acompletion","title":"<code>any_llm.acompletion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, api_timeout=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | ProviderName | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    provider_instance, completion_params = _process_completion_params(\n        model=model,\n        provider=provider,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        api_key=api_key,\n        api_base=api_base,\n        api_timeout=api_timeout,\n        user=user,\n        parallel_tool_calls=parallel_tool_calls,\n        logprobs=logprobs,\n        top_logprobs=top_logprobs,\n        logit_bias=logit_bias,\n        stream_options=stream_options,\n        max_completion_tokens=max_completion_tokens,\n        reasoning_effort=reasoning_effort,\n        client_args=client_args,\n        **kwargs,\n    )\n\n    return await provider_instance.acompletion(completion_params, **kwargs)\n</code></pre>"},{"location":"api/embedding/","title":"Embedding","text":""},{"location":"api/embedding/#embedding","title":"Embedding","text":""},{"location":"api/embedding/#any_llm.embedding","title":"<code>any_llm.embedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def embedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | ProviderName | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = ProviderFactory.split_model_provider(model)\n    else:\n        provider_key = ProviderName.from_string(provider)\n        model_name = model\n\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n\n    provider_instance = ProviderFactory.create_provider(provider_key, api_config)\n\n    return provider_instance.embedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/embedding/#any_llm.aembedding","title":"<code>any_llm.aembedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create an embedding asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aembedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | ProviderName | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = ProviderFactory.split_model_provider(model)\n    else:\n        provider_key = ProviderName.from_string(provider)\n        model_name = model\n\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n\n    provider_instance = ProviderFactory.create_provider(provider_key, api_config)\n\n    return await provider_instance.aembedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#exceptions","title":"Exceptions","text":""},{"location":"api/exceptions/#any_llm.exceptions","title":"<code>any_llm.exceptions</code>","text":"<p>Custom exceptions for any-llm package.</p>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an API key is missing or not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Exception raised when an API key is missing or not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n            env_var_name: Name of the environment variable that should contain the API key\n\n        \"\"\"\n        self.provider_name = provider_name\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError.__init__","title":"<code>__init__(provider_name, env_var_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")</p> required <code>env_var_name</code> <code>str</code> <p>Name of the environment variable that should contain the API key</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n        env_var_name: Name of the environment variable that should contain the API key\n\n    \"\"\"\n    self.provider_name = provider_name\n    self.env_var_name = env_var_name\n\n    message = (\n        f\"No {provider_name} API key provided. \"\n        f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n    )\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported parameter is provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(Exception):\n    \"\"\"Exception raised when an unsupported parameter is provided.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            parameter_name: Name of the parameter that was provided\n            provider_name: Name of the provider that does not support the parameter\n            additional_message: Optional additional information about the error.\n\n        \"\"\"\n        self.parameter_name = parameter_name\n        self.provider_name = provider_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n        if additional_message is not None:\n            message = f\"{message}.\\n{additional_message}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError.__init__","title":"<code>__init__(parameter_name, provider_name, additional_message=None)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of the parameter that was provided</p> required <code>provider_name</code> <code>str</code> <p>Name of the provider that does not support the parameter</p> required <code>additional_message</code> <code>str | None</code> <p>Optional additional information about the error.</p> <code>None</code> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        parameter_name: Name of the parameter that was provided\n        provider_name: Name of the provider that does not support the parameter\n        additional_message: Optional additional information about the error.\n\n    \"\"\"\n    self.parameter_name = parameter_name\n    self.provider_name = provider_name\n\n    message = f\"'{parameter_name}' is not supported for {provider_name}\"\n    if additional_message is not None:\n        message = f\"{message}.\\n{additional_message}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported provider is requested.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(Exception):\n    \"\"\"Exception raised when an unsupported provider is requested.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_key: The provider key that was requested\n            supported_providers: List of supported provider keys\n\n        \"\"\"\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError.__init__","title":"<code>__init__(provider_key, supported_providers)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str</code> <p>The provider key that was requested</p> required <code>supported_providers</code> <code>list[str]</code> <p>List of supported provider keys</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_key: The provider key that was requested\n        supported_providers: List of supported provider keys\n\n    \"\"\"\n    self.provider_key = provider_key\n    self.supported_providers = supported_providers\n\n    message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/list_models/","title":"List Models","text":""},{"location":"api/list_models/#models","title":"Models","text":""},{"location":"api/list_models/#any_llm.list_models","title":"<code>any_llm.list_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List available models for a provider.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def list_models(\n    provider: str | ProviderName,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider.\"\"\"\n    provider_key = ProviderName.from_string(provider)\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n    prov_instance = ProviderFactory.create_provider(provider_key, api_config)\n    return prov_instance.list_models(**kwargs)\n</code></pre>"},{"location":"api/list_models/#any_llm.list_models_async","title":"<code>any_llm.list_models_async(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List available models for a provider asynchronously.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def list_models_async(\n    provider: str | ProviderName,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider asynchronously.\"\"\"\n    provider_key = ProviderName.from_string(provider)\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n    prov_instance = ProviderFactory.create_provider(provider_key, api_config)\n    return await prov_instance.list_models_async(**kwargs)\n</code></pre>"},{"location":"api/responses/","title":"Responses","text":""},{"location":"api/responses/#responses","title":"Responses","text":"<p>Warning</p> <p>This API is experimental and subject to changes based upon our experience as we integrate additional providers. Use with caution.</p>"},{"location":"api/responses/#any_llm.responses","title":"<code>any_llm.responses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, api_timeout=None, user=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def responses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | ProviderName | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = ProviderFactory.split_model_provider(model)\n    else:\n        provider_key = ProviderName.from_string(provider)\n        model_name = model\n\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n\n    provider_instance = ProviderFactory.create_provider(provider_key, api_config)\n\n    responses_kwargs = kwargs.copy()\n    if tools is not None:\n        responses_kwargs[\"tools\"] = prepare_tools(tools)\n    if tool_choice is not None:\n        responses_kwargs[\"tool_choice\"] = tool_choice\n    if max_output_tokens is not None:\n        responses_kwargs[\"max_output_tokens\"] = max_output_tokens\n    if temperature is not None:\n        responses_kwargs[\"temperature\"] = temperature\n    if top_p is not None:\n        responses_kwargs[\"top_p\"] = top_p\n    if stream is not None:\n        responses_kwargs[\"stream\"] = stream\n    if api_timeout is not None:\n        responses_kwargs[\"timeout\"] = api_timeout\n    if user is not None:\n        responses_kwargs[\"user\"] = user\n    if instructions is not None:\n        responses_kwargs[\"instructions\"] = instructions\n    if max_tool_calls is not None:\n        responses_kwargs[\"max_tool_calls\"] = max_tool_calls\n    if parallel_tool_calls is not None:\n        responses_kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n    if reasoning is not None:\n        responses_kwargs[\"reasoning\"] = reasoning\n    if text is not None:\n        responses_kwargs[\"text\"] = text\n\n    return provider_instance.responses(model_name, input_data, **responses_kwargs)\n</code></pre>"},{"location":"api/responses/#any_llm.aresponses","title":"<code>any_llm.aresponses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, api_timeout=None, user=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | ProviderName | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aresponses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | ProviderName | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = ProviderFactory.split_model_provider(model)\n    else:\n        provider_key = ProviderName.from_string(provider)\n        model_name = model\n\n    api_config = ClientConfig(api_key=api_key, api_base=api_base, client_args=client_args)\n\n    provider_instance = ProviderFactory.create_provider(provider_key, api_config)\n\n    responses_kwargs = kwargs.copy()\n    if tools is not None:\n        responses_kwargs[\"tools\"] = prepare_tools(tools)\n    if tool_choice is not None:\n        responses_kwargs[\"tool_choice\"] = tool_choice\n    if max_output_tokens is not None:\n        responses_kwargs[\"max_output_tokens\"] = max_output_tokens\n    if temperature is not None:\n        responses_kwargs[\"temperature\"] = temperature\n    if top_p is not None:\n        responses_kwargs[\"top_p\"] = top_p\n    if stream is not None:\n        responses_kwargs[\"stream\"] = stream\n    if api_timeout is not None:\n        responses_kwargs[\"timeout\"] = api_timeout\n    if user is not None:\n        responses_kwargs[\"user\"] = user\n    if instructions is not None:\n        responses_kwargs[\"instructions\"] = instructions\n    if max_tool_calls is not None:\n        responses_kwargs[\"max_tool_calls\"] = max_tool_calls\n    if parallel_tool_calls is not None:\n        responses_kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n    if reasoning is not None:\n        responses_kwargs[\"reasoning\"] = reasoning\n    if text is not None:\n        responses_kwargs[\"text\"] = text\n\n    return await provider_instance.aresponses(model_name, input_data, **responses_kwargs)\n</code></pre>"}]}